\chapter{Multiple linear regression}
Similarly to simple linear regression where again want to 
to predict values by fitting a linear model to observed data.
But instead of just predicting the value based on one variable we will use a combination of values.

So the goal is to predict the target variable $y_{i}$ given known values of a sample $\bm{x_{i}}$ \\
For example we might want to predict the price $y_{i}$ of a house $i$ given its
\begin{itemize}
\item $x_{i1}$: living area
\item $x_{i2}$: garden size
\item $x_{i3}$: age
\item $x_{i4}$: distance to next bus stop
\item $x_{i5}$: baseline value or dummy variable which will be set to $1$.
\end{itemize}

The last (dummy) feature takes the role of the intercept $b$ that we used in simple linear regression and makes it easier
to work out the math.

We can summarize the values of these features in a vector:
\begin{align}
    \bm{x_{i}} = \begin{pmatrix}
    x_{i1}\\
    x_{i2}\\
    x_{i3}\\
    x_{i4}\\
    x_{i5} =1
    \end{pmatrix}.
\end{align}

The model predictions can be written as:
\begin{align}
\hat{y_{i}} =& w_{1}x_{i1} + w_{2}x_{i2} + w_{3}x_{i3} + w_{4}x_{i4} + w_{5}x_{i5}  \\
            =& \bm{w}^T \bm{x_{i}},
\end{align}
where the entries of the vector $\bm{w}$ should be fitted to a dataset which consists of of $n$ tuples
$((\bm{x}_1, y_1), \hdots, (\bm{x}_1, y_1))$.

In matrix notation we can calculate all the predictions simultaneously:
\begin{align}
    \bm{\hat{y}} = 
    \begin{pmatrix}
        \hat{y_{1}} \\
        \vdots\\
        \hat{y_{n}} 
    \end{pmatrix}  =
    \begin{pmatrix}
        \horzbar & \bm{x}_1 & \horzbar \\
                  & \vdots   &  \\
        \horzbar & \bm{x}_n &  \horzbar \\
    \end{pmatrix} 
    \begin{pmatrix}
        \vertbar \\
        \bm{w}\\
        \vertbar
    \end{pmatrix}=\bm{Xw},
\end{align}
where $x_{ij}$ is j-th feature of sample with index $i$. $\bm{y}$ are the true values.
$m$ is number of features.
The objective is again to minimize the mean squared error,
\begin{align}
    L(\bm{w}) =& \frac{1}{n}(\hat{\bm{y}}-\bm{y})^2 \\
              =& \frac{1}{n}(\bm{X}\bm{w}-\hat{\bm{y}})^2 \\
              =& \frac{1}{n}\sum_{i=1}^{n}(\bm{x_{i}}^T\bm{w}-y_{i})^2 \\ 
              =& \frac{1}{n}\sum_{i=1}^{n}\left(\sum_{j=1}^{m}x_{ij}w_{j}-y_{i}\right)^2.
\end{align}

We again set the gradient w.r.t.\ to the loss to zero to find a minimum in the loss:
\begin{equation*}
  \begin{array}{l}
   \bm{\nabla}L(\bm{w})=
    \begin{pmatrix}
        \pdv{L(\bm{w})}{w_{1}}\\
        \vdots\\
        \pdv{L(\bm{w})}{w_{m}}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \partial_1 L(\bm{w})\\
        \vdots\\
        \partial_m L(\bm{w})
    \end{pmatrix}
  \end{array}
\end{equation*}

The derivative of the loss w.r.t. to the a-th component of $\bm{w}$ is,
\begin{align}
    \pdv{L(\bm{w})}{w_{a}} =& \partial_a L(\bm{w}) \\
    =&\partial_a \frac{1}{n} \sum_{i=1}^{n}\left(\sum_{j=1}^{m}x_{ij}w_{j}-y_{i}\right)^2\\
    =&\frac{1}{n}\sum_{i=1}^{n}2\left(\sum_{j}x_{ij}w_{j}-y_{i}\right)\partial_a \left(\sum_{j}x_{ij}w_{j}-y_{i}\right)\\
    =&\frac{2}{n}\sum_{i} \left(\sum_{j}x_{ij}w_{j}-y_{i}\right)\left(\sum_{j}x_{ij}\partial^a w_{j}\right)\\
    =&\frac{2}{n}\sum_{i} \left(\sum_{j}x_{ij}w_{j}-y_{i}\right)\left(\sum_{j}x_{ij}\partial_{ja}\right)\\
    =&\frac{2}{n} \sum_{i}\left(\sum_{j}x_{ij}w_{j}-y_{i}\right)x_{ia}= 0
\end{align}
This gives us a system of m linear equations, one for each entry of $\bm{w}$.
We can also write this in matrix form,
\begin{align}
    &\sum_{i=1}^{n} x_{ai}^T\left(\sum_{j=1}^{m}x_{ij}w_{j}-y_{i}\right)=0 \\
    &\sum_{i=1}^{n} x_{ai}^T \sum_{j=1}^{m} x_{ij} w_{j} = \sum_{j=1}^{n}x_{ai}^T y_{i}\\
    &\sum_{i=1}^{n} x_{ai}^T [\bm{Xw}]_i = [\bm{X}^T\bm{y}]_a\\
    &[\bm{X}^T\bm{Xw}]_a = [\bm{X}^T\bm{y}]_a\\
    &\bm{X}^T\bm{Xw}  = \bm{X}^T\bm{y}
\end{align}
We would like to solve this equation for $\bm{w}$.
If $\bm{X}^T\bm{X}$ is invertible the solution is 
\begin{align}
    \bm{w}=(\bm{X}^T\bm{X})^{-1} \bm{X}^T \bm{y}
\end{align}

$\bm{X}^T \bm{X}$ is not invertible if there are more features than samples or if columns of $\bm{X}$ are linear dependent. 
We can solve the system of equations using the SVD of $\bm{X}$:
\begin{align}
    \bm{X}=\bm{U\Sigma V}^T    
\end{align}

Inserting this above gives:
\begin{align}
    \bm{X}^T\bm{Xw}  = \bm{X}^T\bm{y} \\
    \bm{V}\bm{\Sigma}^T \bm{U}^T \bm{U\Sigma V}^T \bm{w}  = \bm{V\Sigma}^T\bm{U}^T \bm{y} \\
    \bm{V}^T\bm{V}\bm{\Sigma}^T \bm{\Sigma V}^T \bm{w}  = \bm{V}^T\bm{V\Sigma}^T\bm{U}^T \bm{y} \\
    \bm{\Sigma}^T \bm{\Sigma V}^T \bm{w}  = \bm{\Sigma}^T\bm{U}^T \bm{y} \\
\end{align}

There are different scenarios for the solution depending on the 
whether there are more samples than features and whether the feature columns are linearly independent.

\paragraph{More samples than features and linearly independent feature columns:}
In this case $\bm{\Sigma}$ has the following form where all $\sigma_i>0$:
\begin{align}
    \bm{\Sigma} = 
    \begin{pmatrix}
        \sigma_{1} &        &                \\
                     & \ddots &              \\
                     &        &  \sigma_{m}  \\
        \horzbar     & \bm{0} &  \horzbar    \\
                     & \vdots &              \\
        \horzbar     & \bm{0} &  \horzbar    \\
    \end{pmatrix}_{n \x m}
\end{align}

\begin{align}
    \begin{pmatrix}
        \sigma_{1}^2 &        &               \\
                     & \ddots &               \\
                     &        &  \sigma_{m}^2 \\
    \end{pmatrix}_{m \x m} \bm{V}^T \bm{w} = 
    \begin{pmatrix}
        \sigma_{1} &        &                & \vertbar &        & \vertbar \\
                     & \ddots &              & \bm{0}   & \hdots & \bm{0}   \\
                     &        &  \sigma_{m}  & \vertbar &        & \vertbar \\
    \end{pmatrix}_{m \x n}\bm{U}^T \bm{y} \\
    \bm{w} = 
    \bm{V}
    \begin{pmatrix}
        \sigma_{1}^{-1} &        &                & \vertbar &        & \vertbar \\
                     & \ddots &              & \bm{0}   & \hdots & \bm{0}   \\
                     &        &  \sigma_{m}^{-1}  & \vertbar &        & \vertbar \\
    \end{pmatrix}_{m \x n}\bm{U}^T \bm{y}
\end{align}

\paragraph{More features than samples or linearly dependent features columns:}
More features than samples or linearly dependent features will result in a $\bm{\Sigma}$ that includes all-zero columns.
In this case the equation above will have the following form.
Since $\bm{V}$ is invertible we can solve for $\bm{w}' = \bm{V}^T \bm{w}$.
\begin{align}
    \begin{pmatrix}
        \sigma_{1}^2 &        &               &   & &      &   \\
                     & \ddots &               &   & &      &   \\
                     &        &  \sigma_{l}^2 &   & &      &   \\
                     &        &               & 0 & &      &   \\
                     &        &               &   & \ddots &   \\                     
                     &        &               &   &        & 0 \\                     
    \end{pmatrix}_{m \x m} \bm{w}' = 
    \begin{pmatrix}
        \sigma_{1}   &        &               &   &        &   \\
                     & \ddots &               &   &        &   \\
                     &        &  \sigma_{l}   &   &        &   \\
                     &        &               & 0 &        &   \\
                     &        &               &   & \ddots &   \\  
                     &        &               &   &        & 0 \\
        0            &        &  \hdots       &   &        & 0      \\
        \vdots       &        &               &   &        & \vdots \\
        0            &        &  \hdots       &   &        & 0      \\                     
    \end{pmatrix}_{m \x n}\bm{U}^T \bm{y} \\
\end{align}
We see that this system of equations is non-trivial only for the upper $l$ rows.
The lower rows just give equations of the form $0=0$. This also has the effect that 
the $w`_{l+1}, \hdots, w`_{m}$ are cancelled out of the equation and thus become arbitrary.
We can write the system of equations above for only the first $l$ components:

This gives us the solution for $(w'_{1},\hdots, w'_l)$
\begin{align}    
    \begin{pmatrix}
        \sigma_{1}^2 &        &               \\
                     & \ddots &               \\
                     &        &  \sigma_{l}^2 \\
    \end{pmatrix} 
    \begin{pmatrix}
        w'_1\\
        \vdots \\
        w'_l
    \end{pmatrix} &= 
    \begin{pmatrix}
        \sigma_{1}   &        &               & 0       & \hdots & 0      \\
                     & \ddots &               & \vdots  &        & \vdots \\
                     &        &  \sigma_{l}   & 0       & \hdots & 0      \\                     
    \end{pmatrix}_{l \x n}\bm{U}^T \bm{y} \\
    \begin{pmatrix}
        w'_1\\
        \vdots \\
        w'_l
    \end{pmatrix} &=
    \begin{pmatrix}
        \sigma_{1}^{-1}   &        &               & 0       & \hdots & 0      \\
                     & \ddots &                    & \vdots  &        & \vdots \\
                     &        &  \sigma_{l}^{-1}   & 0       & \hdots & 0      \\                     
    \end{pmatrix}_{l \x n}\bm{U}^T \bm{y}, \\
\end{align}
while $(w'_{l+1},\hdots, w'_m)$ are arbitrary.

Since 
\begin{align}
\bm{w}^T \bm{w} = \bm{w}'^T \bm{V}^T \mathbf{V} \bm{w}'=\bm{w}'^T \bm{w}' = \sum_{i=1}^{m}w_{i}'^2
\end{align}
we get a minimum norm solution for $\bm{w}$ if we choose $w_{i}=0, i \in \{l+1,\hdots, m\}$.
