\chapter{Linear algebra: Singular value decomposition}
The singular value decomposition is a useful way to factorize a matrix. 
Any matrix $\bm{M} \in R^{m \x n}$ can be written as a product of three matrices
\begin{align}
    \underset{(m \x n)}{\bm{M}}  = \underset{(m \x m)}{\bm{U}}\underset{(m \x n)}{\bm{\Sigma}} \underset{(n \x n)}{\bm{V}^T}\\,
\end{align}

where $\bm{U}$ and $\bm{V}$ are orthogonal 
\begin{align}
    \bm{UU}^T = \bm{1}_{m \x m} && \bm{VV}^T = \bm{1}_{n \x n},
\end{align}
and $\bm{\Sigma}$ is diagonal.

Without loss of generality we can assume that $m \geq n$. This would only result in empty columns instead of empty rows of $\bm{\Sigma}$.
The shapes look approximately like this ($5 \x 3$) in the example.
\begin{align}
    \begin{bmatrix}
    &        & \\
    &        & \\
    & \bm{M} & \\
    &        & \\
    &        & 
    \end{bmatrix}    = 
    \begin{bmatrix}
    & &        & & \\
    & &        & & \\
    & & \bm{U} & & \\
    & &        & & \\
    & &        & & 
    \end{bmatrix}_{m \x m}
    \begin{bmatrix}
    \sigma_1 &        &          \\
             & \ddots &          \\
             &        & \sigma_n \\
             &        &          \\
             &        &          
    \end{bmatrix}_{m \x n}
    \begin{bmatrix}
        &          & \\
        & \bm{V}^T & \\
        &          &  
    \end{bmatrix}_{n \x n}
\end{align}

\paragraph{Proof of existence}
We will prove that such a representation always exists.
We know that $\bm{M}^T \bm{M}$ has an orthogonal eigenbasis as it is symmetric, and non-negative eigenvalues as it is positive-definite,
\begin{align}
    \bm{M}^T \bm{M} \bm{v}_i = \alpha_i \bm{v}_i, && \alpha_i \geq 0, &&  \bm{v}_i \cdot \bm{v}_j = \delta_{ij}.
\end{align}
Next we define vectors 
\begin{align}
\bm{u}_i = \frac{\bm{Mv}_i}{\alpha_i}    
\end{align}
and observe that they are also pairwise orthogonal
\begin{align}
    \bm{u}_i \bm{u}_j = \frac{\bm{v}_i^T \bm{M}^T \bm{Mv}_i}{\sqrt{\alpha_i \alpha_j}} = \frac{\bm{v}_i^T \bm{v}_i \alpha_i}{{\sqrt{\alpha_i \alpha_j}}} = 
    \frac{\delta_{ij} \alpha_i}{{\sqrt{\alpha_i \alpha_j}}} = 
    \frac{\delta_{ij} \alpha_i}{{\sqrt{\alpha_i \alpha_i}}} = \delta_{ij}
\end{align}

We also get that the $\bm{u}_i$ are eigenvectors of $\bm{MM}^T$, 
\begin{align}
     (\bm{M}\bm{M})^T \bm{u}_i
    =(\bm{M}\bm{M})^T \bm{Mv}_i \frac{1}{\sqrt{\alpha_i}} 
    =\bm{M}(\bm{M}^T \bm{M}) \bm{v}_i \frac{1}{\sqrt{\alpha_i}} 
    = \alpha_i \bm{M} \bm{v}_i  \frac{1}{\sqrt{\alpha_i}} 
    = \alpha_i \bm{u}_i
\end{align}

Next we assume that$\bm{M}^T \bm{M}$ has $l$  non-zero eigenvalues.
We take the respective eigenvectors $\bm{v}_1, \hdots, \bm{v}_l$  and compute the corresponding $\bm{u}_i$ 
simultaneously, while using $\sigma_i=\alpha_i^{-\frac{1}{2}}$,
\begin{align}
    \begin{pmatrix}
    \vertbar &        & \vertbar \\
    \bm{u}_1 & \hdots & \bm{u}_l \\
    \vertbar &        & \vertbar 
    \end{pmatrix} =&
    \bm{M}
    \begin{pmatrix}
    \vertbar &        & \vertbar \\
    \bm{v}_1 & \hdots & \bm{v}_l \\
    \vertbar &        & \vertbar 
    \end{pmatrix}
    \begin{pmatrix}
    \sigma_1 &        &          \\
             & \ddots &          \\ 
             &        & \sigma_l
    \end{pmatrix} \\
    \begin{pmatrix}
    \vertbar &        & \vertbar \\
    \bm{u}_1 & \hdots & \bm{u}_l \\
    \vertbar &        & \vertbar 
    \end{pmatrix}
    \begin{pmatrix}
        \sigma_1 &        &          \\
                 & \ddots &          \\ 
                 &        & \sigma_l
        \end{pmatrix}
    =&
    \bm{M}
    \begin{pmatrix}
    \vertbar &        & \vertbar \\
    \bm{v}_1 & \hdots & \bm{v}_l \\
    \vertbar &        & \vertbar 
    \end{pmatrix} \\
    \begin{pmatrix}
    \vertbar &        & \vertbar \\
    \bm{u}_1 & \hdots & \bm{u}_l \\
    \vertbar &        & \vertbar 
    \end{pmatrix}
    \begin{pmatrix}
        \sigma_1 &        &          \\
                 & \ddots &          \\ 
                 &        & \sigma_l
    \end{pmatrix}
    \begin{pmatrix}
        \horzbar & \bm{v}_1 & \horzbar \\
                  & \vdots   &  \\
        \horzbar & \bm{v}_l &  \horzbar \\
    \end{pmatrix} 
    =&
    \bm{M}
    \begin{pmatrix}
    \vertbar &        & \vertbar \\
    \bm{v}_1 & \hdots & \bm{v}_l \\
    \vertbar &        & \vertbar 
    \end{pmatrix}
    \begin{pmatrix}
        \horzbar & \bm{v}_1 & \horzbar \\
                  & \vdots   &  \\
        \horzbar & \bm{v}_l &  \horzbar \\
    \end{pmatrix} 
\end{align}

To get to our desired factorization and isolate $\bm{M}$ we would like to get somehow get rid of the last to matrices on the right.
However while $\bm{V}^T \bm{V} = \bm{1}_{l \x l}$, $\bm{V}^T \bm{V} = \bm{1}_{n \x n}$ does not hold in general, and we can't easily 
cancel it out.
We know however, that 
\begin{align}
    \bm{v}_i^T \bm{M}^T \bm{M} \bm{v_i} =  \bm{v}_i^T \bm{v}_i \alpha_i \\
    \alpha_i = 0 \ \Rightarrow \ \bm{Mv}_i = 0
\end{align}

This allows us to do the following, where the first equality holds because we use the full set of eigenvectors $n$ eigenvectors,
\begin{align}
    &\bm{M} = \bm{M}
    \begin{pmatrix}
    \vertbar &        & \vertbar & \red{\vertbar    } & \red{        }& \red{ \vertbar }\\
    \bm{v}_1 & \hdots & \bm{v}_l & \red{\bm{v}_{l+1}} & \red{ \hdots }& \red{ \bm{v}_n }\\
    \vertbar &        & \vertbar & \red{\vertbar    } & \red{        }& \red{ \vertbar }
    \end{pmatrix}
    \begin{pmatrix}
        \horzbar & \bm{v}_1   & \horzbar \\
                 & \vdots     &          \\
        \horzbar & \bm{v}_l   & \horzbar \\
        \red{\horzbar} & \red{\bm{v}_l+1} & \red{\horzbar} \\
                       & \red{\vdots    } &                \\
        \red{\horzbar} & \red{\bm{v}_n  } & \red{\horzbar} \\
    \end{pmatrix} \\
    =&
    \begin{pmatrix}
    \vertbar       &        & \vertbar       & \red{\vertbar    }       & \red{        }& \red{ \vertbar }       \\
    \bm{M}\bm{v}_1 & \hdots & \bm{M}\bm{v}_l & \red{\bm{M}\bm{v}_{l+1}} & \red{ \hdots }& \red{\bm{M} \bm{v}_n } \\
    \vertbar       &        & \vertbar       & \red{\vertbar    }       & \red{        }& \red{ \vertbar }
    \end{pmatrix}
    \begin{pmatrix}
        \horzbar & \bm{v}_1   & \horzbar \\
                 & \vdots     &          \\
        \horzbar & \bm{v}_l   & \horzbar \\
        \red{\horzbar} & \red{\bm{v}_l+1} & \red{\horzbar} \\
                       & \red{\vdots    } &                \\
        \red{\horzbar} & \red{\bm{v}_n  } & \red{\horzbar} \\
    \end{pmatrix} \\
    =&
    \begin{pmatrix}
    \vertbar       &        & \vertbar       & \red{\vertbar} & \red{        } & \red{\vertbar} \\
    \bm{M}\bm{v}_1 & \hdots & \bm{M}\bm{v}_l & \red{\bm{0}  } & \red{ \hdots } & \red{\bm{0}}   \\
    \vertbar       &        & \vertbar       & \red{\vertbar} & \red{        } & \red{\vertbar}
    \end{pmatrix}
    \begin{pmatrix}
        \horzbar & \bm{v}_1   & \horzbar \\
                 & \vdots     &          \\
        \horzbar & \bm{v}_l   & \horzbar \\
        \red{\horzbar} & \red{\bm{v}_l+1} & \red{\horzbar} \\
                       & \red{\vdots    } &                \\
        \red{\horzbar} & \red{\bm{v}_n  } & \red{\horzbar} \\
    \end{pmatrix}\\
    =& \bm{M}\begin{pmatrix}
    \vertbar       &        & \vertbar       & \red{\vertbar} & \red{        } & \red{\vertbar} \\
    \bm{v}_1 & \hdots & \bm{v}_l & \red{\bm{0}  } & \red{ \hdots } & \red{\bm{0}}   \\
    \vertbar       &        & \vertbar       & \red{\vertbar} & \red{        } & \red{\vertbar}
    \end{pmatrix}
    \begin{pmatrix}
        \horzbar & \bm{v}_1   & \horzbar \\
                 & \vdots     &          \\
        \horzbar & \bm{v}_l   & \horzbar \\
        \red{\horzbar} & \red{\bm{v}_l+1} & \red{\horzbar} \\
                       & \red{\vdots    } &                \\
        \red{\horzbar} & \red{\bm{v}_n  } & \red{\horzbar} \\
    \end{pmatrix} \\
    =&
    \bm{M} \sum_{i=1}^l \bm{v}_i \bm{v}_i^T + \sum_{i=l+1}^n \bm{0} \ \bm{v}_i^T =
    \bm{M} \sum_{i=1}^l \bm{v}_i \bm{v}_i^T  \\
    =&     \bm{M}
    \begin{pmatrix}
    \vertbar &        & \vertbar \\
    \bm{v}_1 & \hdots & \bm{v}_l \\
    \vertbar &        & \vertbar 
    \end{pmatrix}
    \begin{pmatrix}
        \horzbar & \bm{v}_1 & \horzbar \\
                  & \vdots   &  \\
        \horzbar & \bm{v}_l &  \horzbar \\
    \end{pmatrix}.
\end{align}
This is just the rhs. of \eqref{?} and we get that 
\begin{align}
    \bm{M} = \begin{pmatrix}
    \vertbar &        & \vertbar \\
    \bm{u}_1 & \hdots & \bm{u}_l \\
    \vertbar &        & \vertbar 
    \end{pmatrix}
    \begin{pmatrix}
        \sigma_1 &        &          \\
                 & \ddots &          \\ 
                 &        & \sigma_l
        \end{pmatrix}
    \begin{pmatrix}
        \horzbar & \bm{v}_1 & \horzbar \\
                  & \vdots   &  \\
        \horzbar & \bm{v}_l &  \horzbar \\
    \end{pmatrix} 
\end{align}
This is the reduced singular value decomposition of matrix. 

