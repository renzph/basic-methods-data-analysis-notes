\chapter{Summarizing univariate data}
There are n observations, where n is the number of students.
It is hard to make sense of this data. 
First idea is analyzing the data: 
Calculate size of a typical person by so called "central tendency".

\begin{center}
    $ X=(160, 163, 156, \ldots, 183) = (x_{1},\ldots, x_{n})$
\end{center}
    
\section {Arithmetic mean}

\begin{align}
\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_{i} = \frac{1}{n}\left(x_{1}+x_{2}+\ldots+x_{n}\right)
\end{align}

Not a robust statistic because it is heavily influenced by outliers.
\paragraph{Example:} 100 people live in a village. Everybody is earning 100 \texteuro \hspace{0.1cm} a month, but one person earns 1 billion \texteuro \hspace{0.1cm} a month. The average wage is 10099 \texteuro, but the typical person is not earning that much. 
    
    
\subsection {Median}
More robust measure of central tendency.
The median splits the data sample in half. 

\begin{align}
      median(x) = 
      \begin{cases}
          x_{\frac{n+1}{2}} & \text{if n is odd}\\
          \frac{1}{2}(x_{n/2}+x_{n/2+1}) & \text{if n is even}
      \end{cases}
\end{align}       
    
    
\paragraph{Example:}

\begin{align}
    x &= (100,110,120,200,300,1000000)\\
    median(x) &= \frac{120+200}{2} = 160
\end{align}
    
\subsection {Quantiles}
Cut points that divide sample into equally sized slices. 
Quantiles divide the data into four regions.
\begin{itemize}
  \item $Q_{1}$: the first quantile, middle number between minimum and median
  \item $Q_{2}$: the second quantile, median  
  \item $Q_{3}$: the third quantile, middle number between median and maximum
\end{itemize}


Similarly deciles split the data into ten and percentiles into 100 slices.
    
\subsection {Boxplot}
Popular use of quantiles. It gives information both about the central tendency and spread of the data.

% use figure env
% \begin{figure}
% \includegraphics[scale=0.2]{boxplot.jpg}
% \end{figure}
    
Whisker options:
\begin{itemize}
\item minimum / maximum
\item $l=1.5 \ (Q_{3}-Q_{1})$, values outside of the whiskers are called outliers and drawn individually
\item e.g. can show the $5^{th}$ and $95^{th}$ percentile
\end{itemize}
    
    
\subsection {Histogram}
More complex than boxplot but better suited for multi-modal distributions, i.e. distributions with several peaks:
\begin{itemize}
  \item Divide data range into equally sized intervals
  \item Count number of samples in each interval
\end{itemize}
    
% \begin{figure}
% \includegraphics[scale=0.6]{mulitmodal1.png}
% \end{figure}
The interval size has to be chosen suitably.

\begin{itemize}
    \item Too small $\Rightarrow$ all counts are either one or zero. 
    \item Too large $\Rightarrow$ no gained information.
\end{itemize}
    
\subsection {Measuring spread of data}
A common way to measure the spread of data is to calculate the average distance to the mean value.
A simple idea idea would be to use:

\begin{align}
   & \frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x}) \\
  =& \frac{1}{n}\left(\sum_{i=1}^{n}x_{i} - \sum_{i=1}^{n}\bar{x}\right)\\
  =& \frac{1}{n}\left(\sum_{i=1}^{n}x_{i} - n\bar{x}\right) \\
  =& \frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right) - \bar{x} \\
  =& \bar{x}-\bar{x} = 0
\end{align}
    
\subsection {Mean absolute deviation}
\begin{align}
  \text{MAD}(x) = \frac{1}{n}\sum_{i=1}^{n}|(x_{i}-\bar{x})|
\end{align}
    
Exercise: Find a that minimizes
\begin{align}
    \frac{1}{n}\sum_{i=1}^{n}|(x_{i}-a)|
\end{align}
    
    
\subsection {Variance: Mean squared deviation}

\begin{align}
\sigma^2(x) = \text{var}(x) = \frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^2
\end{align}

Alternative way to write this:
\begin{align}
\text{var}(x) =& \frac{1}{n}\left(\sum_{i=1}^{n}(x_{i}^2 - 2x_{i}\bar{x}+\bar{x}^2)  \right) =\\
         =& \frac{1}{n}\left(\sum_{i=1}^{n} x_{i}^2 - 2\bar{x} \ \frac{1}{n}\sum_{i=1}^{n}x_{i}+\bar{x}^2 \right) =\\
         =& \frac{1}{n}\left(\sum_{i=1}^{n} x_{i}^2 - 2\bar{x}^2+\bar{x}^2 \right) =\\
         =& \frac{1}{n}\left(\sum_{i=1}^{n} x_{i}^2 - \bar{x}^2 \right) = \\ 
         =& \left(\frac{1}{n} \sum_{i=1}^{n} x_{i}^2 \right) - \bar{x}^2  = \\ 
         =& \bar{x^2} - \bar{x}^2
\end{align}
This means the variance can be calculated using the mean squared value and the mean of the data.

\paragraph{Exercise:} Find the value of $a$ that minimizes 
\begin{align}
\frac{1}{n}\sum_{i=1}^{n}(x_{i}-a)^2
\end{align}


\subsection {Standard deviation}

\begin{equation*}
\sigma(x) = \text{std}(x) = \sqrt{\text{var}(x)} =\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^2}
\end{equation*}


\section{Summarizing Bivariate Data}
When dealing with bivariate data we have two values for each sample.
For example, we measure weight ($x_i$) and height ($y_i$) of animal with index $i$.
Our dataset is then given by tuples of data:
\begin{align}
  z = (z_{1},z_{2},\ldots,z_{n}) =((x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3}),\ldots,(x_{n},y_{n}))
\end{align}

\subsection{Scatterplot}
    
\subsection {Covariance}
To analyze this kind of data we can again look at the central tendencies and 
spreads of the single features.
Additionally we can analyze how the two features they vary together.

The covariance between $x$ and $y$ is defined as

\begin{align}
s_{xy}=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y}) = s_{yx}
\end{align}
If the covariance is positive then on average then on avarage a 
positive deviation from the mean in one feature leads to one in the other and vice versa.
A negative value says that a positive deviation from the mean in one feature leads to a negative one in the other.

Similar to the variance we can also write the covariance as 
    
\begin{align}
    s_{xy} &= \frac{1}{n} \sum_{i=1}^{n} (x_{i}y_{i}-x_{i}\bar{y}-\bar{x}y_{i}+\bar{x}\bar{y}) \\
           &= \frac{1}{n} \sum_{i=1}^{n} x_{i}y_{i}-\bar{y}\frac{1}{n}\sum_{i=1}^{n}x_{i}-\bar{x}\frac{1}{n}\sum_{i=1}^{n}y_{i}+\bar{x}\bar{y} \\
           &= \frac{1}{n} \sum_{i=1}^{n} x_{i}y_{i}-\bar{x}\bar{y}-\bar{x}\bar{y}+\bar{x}\bar{y}\\
           &= \frac{1}{n} \sum_{i=1}^{n} x_{i}y_{i}-\bar{x}\bar{y}
\end{align}



The covariance of a feature with itself recovers the variance.
The variance on covariance are often combined in the so called covariance matrix
\begin{equation*}
\bm{\Sigma} = 
\begin{pmatrix}
s_{xx} & s_{xy} \\
s_{yx} & s_{yy} 
\end{pmatrix}
\end{equation*}
It gives information about spread of data ($s_{xx},s_{yy}$ and how they vary together.

\newcommand*\mean[1]{\overline{#1}}

The data is often written in form of a matrix 
\begin{align}
\bm{Z} = 
\begin{pmatrix}
x_{1} & x_{2} & \hdots & x_n \\
y_{1} & y_{2} & \hdots & y_{n} \\
\end{pmatrix} 
\end{align}
and by taking the row-wise means we get the vector of means
\begin{equation*}
\mean{\bm{z}} = 
\begin{pmatrix}
\bar{x} \\
\bar{y} 
\end{pmatrix}
\end{equation*}

We can take another look at the covariance matrix and try to write it in the terms defined above.
\begin{align}
  \bm{\Sigma} =& \frac{1}{n} \begin{pmatrix}
    \sum_{i=1}^{n}x_{i}^2 - \mean{x}^2      & \sum_{i=1}^{n}x_{i}y_{i} - \mean{x} \ \mean{y} \\
    \sum_{i=1}^{n}y_{i}x_{i} - \mean{x} \ \mean{y} & \sum_{i=1}^{n}y_{i}y_{i} -  \mean{y}^2
  \end{pmatrix} = \\
=& \frac{1}{n} \red{\begin{pmatrix}
  \sum_{i=1}^{n}x_{i}^2    & \sum_{i=1}^{n}x_{i}y_{i} \\
  \sum_{i=1}^{n}y_{i}x_{i} & \sum_{i=1}^{n}y_{i}y_{i} 
\end{pmatrix}} - 
\blue{\begin{pmatrix}
  \mean{x}^2        &  \mean{x} \mean{y} \\
  \mean{x} \mean{y} &  \mean{y}^2
\end{pmatrix}} = \\
=& \frac{1}{n}\red{
\begin{pmatrix}
    x_{1} & x_{2} & \hdots & x_{n} \\
    y_{1} & y_{2} & \hdots & y_{n}
\end{pmatrix}
\begin{pmatrix}
    x_{1} & y_{1} \\
    x_{2} & y_{2} \\
    \vdots  & \vdots \\
    x_{n} & y_{n}
\end{pmatrix}} - \blue{\begin{pmatrix}
  \mean{x} \\
  \mean{y}
\end{pmatrix} \begin{pmatrix}
  \mean{x} &
  \mean{y}
\end{pmatrix}} = \\
=& \frac{1}{n}\red{\bm{ZZ}^T}- \blue{\mean{\bm{z}} \ \mean{\bm{z}}^T}
\end{align}

\begin{equation*}
S_{xy}=\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}-\bar{x}\bar{y}
\end{equation*}
    
    
\subsection {Pearson Correlation Coefficient}
The pearson correlation coefficient 
\begin{align}
    r_{xy} = \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_{i}-\bar{y})^2}}
\end{align}
can be seen as a normalized covariance.
It ranges from $-1$ to $1$.
  To see this, we view the quantities above as vectors.
\begin{align}
  u_{i} = (x_{i}-\mean{x}) \\
  v_{i} = (y_{i}-\mean{y})
\end{align}
  
  
\begin{align}
      \bm{u} = 
        \begin{pmatrix}
      x_{i} \\
      \vdots \\
      x_{n}
        \end{pmatrix}
        -\bar{x}
        \hspace{40pt}
      \bm{v} =  
        \begin{pmatrix}
      y_{i} \\
      \vdots \\
      y_{n}
        \end{pmatrix}
        -\bar{y}\\
\end{align}
  
\begin{equation*}
r_{xy} = \frac{\bm{u} \cdot \bm{v}}{\norm{\bm{u}}\norm{\bm{v}}} = cos_{\alpha}
\end{equation*}

    
\begin{align}
  r_{xy} =
    \begin{cases}
      1  & \text{if $x_{i}= ay_{i}+b$ with $a>0$} \\
      -1 & \text{if $x_{i}= -ay_{i}+b$ with $a>0$}
    \end{cases}       
\end{align}
    
For proof see Cauchy-Schwarz inequality.
    
\subsection {Linear regression}
In linear regression our goal is to predict the value 
of one variable given that of another using a linear relation.
Given the data $\left\{(x_{1},y_{1}),\ldots, (x_{n},y_{n})\right\}$
we want to find a function that predicts the value of $y_i$ given that of $x_i$.
This prediction should be made by the model
\begin{equation*}
  \hat{y}_i = wx_i+b
\end{equation*}
where $w$ and $b$ are parameters that have to be adjusted to fit the data.
A simple example would be to predict the weight of a dog given its height.
If all the data points would lie on a straight line we could easily calculate the values
of $w$ and $b$ that fit the data.
However, if this is not possible we can try to find parameters that 
minimize the mean squared error:

\begin{align}
  L(w,b) =& \frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^2 \\
         =& \frac{1}{n}\sum_{i=1}^{n}(y_{i}-(wx_{i}+b))^2
\end{align}

To find a minimum we first set the gradient of the loss w.r.t. to the parameters to zero.

\begin{equation*}
  \bm{\nabla} L(w,b) =  
  \begin{pmatrix}
      \frac{\partial L(w,b)}{\partial w} \\
      \frac{\partial L(w,b)}{\partial b} 
  \end{pmatrix}
  =
  \begin{pmatrix}
      0 \\
      0 
  \end{pmatrix}
\end{equation*}

This gives us a system of linear equations:
\begin{align}
  \frac{\partial L}{\partial w} =& \frac{1}{n}\sum_{i=1}^{n}-2(y_{i}-(wx_{i}+b))x_{i}\\
  \frac{\partial L}{\partial b} =& \frac{1}{n}\sum_{i=1}^{n}-2(y_{i}-(wx_{i}+b))
\end{align}

Solving for $w$ and $b$:
\begin{align}
  &\frac{1}{n}\left(\sum_{i=1}^{n}-2(y_{i}-(wx_{i}+b))\right)=0\\
  &\frac{1}{n}\sum_{i=1}^{n}y_{i}-w\frac{1}{n}\sum_{i=1}^{n}x_{i}-b=0\\
  &\bar{y}-w\bar{x} =b
\end{align}

With the result for $b$ we can solve for $w$:
\begin{align}
    & \frac{1}{n}\sum_{i=1}^{n}-2(y_{i}-(wx_{i}+b))=0 \\
    & \frac{1}{n}\sum_{i=1}^{n}-2(y_{i}-(wx_{i}+b))x_{i}=0\\
    & \frac{1}{n}\sum_{i=1}^{n}y_{i}x_{i}-\bar{x}\bar{y}+\bar{x}\bar{y}-w(\frac{1}{n}\sum_{i=1}^{n}x_{i}^2-\bar{x}^2+\bar{x}^2)-b\frac{1}{n}\sum_{i=1}^{n}x_{i} \\
    & = s_{xy} + \bar{x}\bar{y}-w s_{x}-w\bar{x}^2-b\bar{x}\\
    & \mid b=\bar{y}-w\bar{x}\mid\\
    & = s_{xy}+\bar{x}\bar{y}-w s_{x}-w\bar{x}^2-\bar{x}\bar{y}+w\bar{x}^2\\
    & = s_{xy} -w s_{x}\\
    & =0
\end{align}

Thus we get 
\begin{align}
  s_{x} w = s_{xy}
\end{align}
and
\begin{align}
w =& \begin{cases}
    \frac{s_{xy}}{s_{x}} & \text{if $s_{x} \neq 0$ }\\
    \text{arbitrary} & \text{if $s_{x}$ = 0 }
  \end{cases}\\
  b=&\bar{y}-w\bar{x} 
\end{align}

Next we want to check if this corresponds to a minimum. We do so 
by checking the Hessian matrix is positive-definite.
The first derivatives are:
\begin{align}
    \frac{\partial L}{\partial w} &= \frac{1}{n}\sum_{i=1}^{n}-2(y_{i}-(wx_{i}+b))x_{i}\\
    \frac{\partial L}{\partial b} &= \frac{1}{n}\sum_{i=1}^{n}-2(y_{i}-(wx_{i}+b))\\
\end{align}

Further differentiation gives:
\begin{align}
    \pdv[2]{L}{w}                 &= -\frac{2}{n}\sum_{i=1}^{n}-x_{i}^2 = 2 \mean{x^2} \\
    \pdv{L}{w}{b}                 &= \frac{2}{n}\sum_{i=1}^{n}x_{i} = 2\bar{x}\\
    \pdv[2]{L}{b}                 &= 2
\end{align}

In general it holds that 
\begin{align}
  \frac{\partial}{\partial b} \left(\frac{\partial L}{\partial w}\right)= \frac{\partial}{\partial w} \left(\frac{\partial L}{\partial b}\right),
\end{align}
which lets us save some computation.

This gives us:
\begin{equation*}
  H(L) = 2 
  \begin{pmatrix}
      \bar{x^2} & \bar{x} \\
      \bar{x} & 1 
  \end{pmatrix}
\end{equation*}
If this matrix is strictly positive definite
we have indeed obtained a local minimum.

\begin{align}
  &\begin{pmatrix}
    v_{1}  & v_{2}
  \end{pmatrix}
  \begin{pmatrix}
    \mean{x^2}  & \mean{x} \\
    \mean{x}  & 1
  \end{pmatrix}
  \begin{pmatrix}
    v_{1} \\
    v_{2}
  \end{pmatrix}=\\
  &v_{1}^2\mean{x^2} + 2v_{1}v_{2} \mean{x}+v_{2}^2 \geq \\
  & v_{1}^2\mean{x}^2 + 2v_{1}v_{2}\bar{x}+v_{2}^2 = \\
  &(v_{1}\bar{x}+v_{2})^2\geq0
\end{align}
where we have used 
\begin{equation}
  \mean{x^2}-\mean{x}^2 = \text{var}(x)\geq 0 \Rightarrow
  \mean{x^2} \geq \mean{x}^2
\end{equation}
if $\text{var}(x)>0$ this becomes a strict inequality which means that we get a unique minimum.
Recall that when we have no variance in the $x_i$ the value for $w$ was arbitrary.

Thus we have established a solution for the linear regression problem.
